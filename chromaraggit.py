# -*- coding: utf-8 -*-
"""chromaRAGgit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q89y3kYEfZGrpP6yMb_Mizmy0fxTOqmf
"""

#installation of previous version of openai
!pip install openai==0.28

# Install required packages
!pip install chromadb pymupdf python-pptx openpyxl

!pip install boto3

import os

aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')

# Download required NLTK data for processing language:
#punkt -> sentence splitting, averaged_perceptron_tagger -> pos, stopwords
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

import os
import boto3
from datetime import timezone

def download_pdfs_from_s3_prefixes(bucket_name, prefixes, local_folder):
    s3 = boto3.client('s3')
    os.makedirs(local_folder, exist_ok=True)
    for prefix in prefixes:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        for obj in response.get('Contents', []):
            key = obj['Key']
            if key.endswith('.pdf'):
                filename = os.path.basename(key)
                local_path = os.path.join(local_folder, filename)
                s3_last_modified = obj['LastModified'].replace(tzinfo=timezone.utc).timestamp()
                if (not os.path.exists(local_path)) or (os.path.getmtime(local_path) < s3_last_modified):
                    s3.download_file(bucket_name, key, local_path)
                    print(f"Downloaded {key} to {local_path}")
                else:
                    print(f"Skipped {key}, local file is up to date.")

# Usage:
AWS_BUCKET_NAME = 'hcmbotknowledgesource'
local_pdf_folder = '/content/pdfs2'

# First run:
#download_pdfs_from_s3_prefixes(AWS_BUCKET_NAME, ['slack_pdfs/', 'base_pdfs/'], local_pdf_folder)

# Later runs:
download_pdfs_from_s3_prefixes(AWS_BUCKET_NAME, ['slack_pdfs/'], local_pdf_folder)

import os
import openai

openai.api_key = os.environ["OPENAI_API_KEY"]

#Step 1: Extracting the text
import fitz  # PyMuPDF
from pptx import Presentation
import openpyxl

# Extract text from PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Extract text from PPT
def extract_text_from_ppt(ppt_path):
    prs = Presentation(ppt_path)
    text = ""
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return text

# Extract text from Excel
def extract_text_from_excel(excel_path):
    wb = openpyxl.load_workbook(excel_path)
    sheet = wb.active
    text = ""
    for row in sheet.iter_rows():
        for cell in row:
            text += str(cell.value) + "\n"
    return text

#Step 2: Text preprocessing and embedding
import openai
import numpy as np
import nltk
nltk.download('punkt')

from google.colab import userdata
userdata.get('OPENAI_API_KEY')

#replace null characters, converts to ASCII, removes double spaces and newlines
def clean_text(text):
    text = text.replace("\x00", "")
    text = text.encode("ascii", "ignore").decode()
    text = " ".join(text.split())
    # Convert to lowercase during cleaning
    text = text.lower()
    return text

#OG chunking

#tokenizes text into sentences, if current chunk has space for new sentence add, else new chunk
#keeps overlap sentences to keep context
def split_text_into_chunks(text, max_tokens=500, overlap=100):
    # Ensure text is lowercase before splitting
    text = text.lower()
    sentences = nltk.tokenize.sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length > max_tokens:
            chunk_text = ' '.join(current_chunk)
            chunks.append(chunk_text)

            overlap_words = chunk_text.split()[-overlap:]
            current_chunk = [' '.join(overlap_words), sentence]
            current_length = len(overlap_words) + sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

#sends small batches of text to openai to embed
def generate_embeddings(texts, batch_size=10):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        try:
            response = openai.Embedding.create(
                model="text-embedding-3-small",
                input=batch
            )
            embeddings.extend([embedding["embedding"] for embedding in response["data"]])
        except Exception as e:
            print(f"Error generating embeddings for batch {i}-{i+batch_size}: {e}")
    return embeddings

import nltk
nltk.download('punkt_tab')

# This will store document chunks and their embeddings
document_store = {}

def process_and_upload_from_folder(folder_path, doc_type):
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if filename.endswith(".pdf") and doc_type == "pdf":
            text = extract_text_from_pdf(file_path)
        elif filename.endswith(".pptx") and doc_type == "ppt":
            text = extract_text_from_ppt(file_path)
        elif filename.endswith(".xlsx") and doc_type == "excel":
            text = extract_text_from_excel(file_path)
        else:
            continue

        # Clean and convert to lowercase
        clean_doc_text = clean_text(text)
        text_chunks = split_text_into_chunks(clean_doc_text)

        # Generate embeddings for lowercase text
        embeddings = generate_embeddings(text_chunks)

        # Store the lowercase chunks
        document_store[filename.lower()] = {  # Store filename in lowercase
            "chunks": text_chunks[:len(embeddings)],
            "embeddings": embeddings,
            "source": doc_type,
            "original_filename": filename  # Keep original filename for display
        }
        print(f"Uploaded {filename} ({doc_type.upper()}) to the memory store.")


def cosine_similarity(vec1, vec2):
    vec1_norm = np.linalg.norm(vec1)
    vec2_norm = np.linalg.norm(vec2)

    if vec1_norm == 0 or vec2_norm == 0:
        return 0.0

    return np.dot(vec1, vec2) / (vec1_norm * vec2_norm)

def is_relevant_content(chunk, query):
    """Check if chunk contains actual relevant information rather than metadata."""
    # Skip chunks that are mostly version numbers or deployment info
    if chunk.count(':v') > 3 or chunk.count('-') > 10:
        return False

    # Extract key terms from query (excluding common words)
    query_terms = set(term.lower() for term in query.split()
                     if term.lower() not in {'how', 'what', 'when', 'where', 'do', 'does', 'is', 'are', 'the'})

    # Check if chunk contains any query terms
    chunk_lower = chunk.lower()
    terms_found = sum(1 for term in query_terms if term in chunk_lower)

    return terms_found > 0

def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = generate_embeddings([query])[0]
    similarities = []

    for doc_name, doc_data in document_store.items():
        for chunk, chunk_embedding in zip(doc_data["chunks"], doc_data["embeddings"]):
            similarity = cosine_similarity(query_embedding, chunk_embedding)
            similarities.append((chunk, similarity, doc_name))

    relevant_chunks = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]
    return [(chunk, doc_name) for chunk, _, doc_name in relevant_chunks]

def chat_with_assistant(query):
    relevant_chunks = retrieve_relevant_chunks(query)
    print(f"Relevant chunks for query '{query}':")
    for chunk, doc in relevant_chunks:
        print(f"Source ({doc}): {chunk}")
    context = "\n\n".join([f"Source ({doc}): {chunk}" for chunk, doc in relevant_chunks])

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": """You are a precise assistant that answers questions based strictly on the provided context.
                Rules:
                1. Use ONLY information from the context
                2. Keep exact terminology and steps from the source
                3. If multiple sources have different information, specify which source you're using
                4. If information isn't in the context, say "I don't have enough information"
                5. For procedures, list exact steps in order
                6. Include specific buttons, links, and UI elements mentioned in the source"""
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuestion: {query}"
            }
        ],
        temperature=0.3,
        max_tokens=1500
    )

    return response.choices[0].message.content.strip()


# Process documents
def process_and_upload_all():
    process_and_upload_from_folder(local_pdf_folder, "pdf")
    #process_and_upload_from_folder(ppt_folder, "ppt")
    #process_and_upload_from_folder(excel_folder, "excel")

# Run the processing
process_and_upload_all()

import pickle

#here is where we are processing the documents once and storing them so the quieries from steamlit run multiple, but doc processing is one time
# Serialize the document store to a .pkl file
with open('s3_document_store.pkl', 'wb') as f:
    pickle.dump(document_store, f)

print("Document Store Contents:", document_store)